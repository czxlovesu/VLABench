{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation on different action policies, e.g. VLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "项目根目录已设置为: /home/vla/Downloads/VLABench\n",
      "VLABENCH_ROOT 环境变量已设置为: /home/vla/Downloads/VLABench/VLABench\n"
     ]
    }
   ],
   "source": [
    "# 设置项目路径和环境变量\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 根据当前工作目录推断项目根目录\n",
    "# 正确的项目根目录应该指向VLABench/VLABench\n",
    "project_root = \"/home/vla/Downloads/VLABench\"\n",
    "vlabench_root = \"/home/vla/Downloads/VLABench/VLABench\"  # 这是实际的VLABench包根目录\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# 设置 VLABENCH_ROOT 环境变量指向VLABench包的根目录\n",
    "os.environ[\"VLABENCH_ROOT\"] = vlabench_root\n",
    "\n",
    "\n",
    "print(f\"项目根目录已设置为: {project_root}\")\n",
    "print(f\"VLABENCH_ROOT 环境变量已设置为: {os.getenv('VLABENCH_ROOT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VLABench.evaluation.evaluator import Evaluator\n",
    "from VLABench.evaluation.model.policy.openvla import OpenVLA\n",
    "from VLABench.evaluation.model.policy.base import RandomPolicy\n",
    "from VLABench.tasks import *\n",
    "from VLABench.robots import *\n",
    "\n",
    "demo_tasks = [\"select_fruit\"]\n",
    "unseen = True\n",
    "save_dir = \"/home/vla/Downloads/VLABench/logs\" \n",
    "\n",
    "\n",
    "\n",
    "model_ckpt = \"openvla/openvla-7b\"  # 使用基础oepnVLA\n",
    "lora_ckpt = None  # 如果没有LoRA权重就设为None\n",
    "# model_ckpt = \"/remote-home1/pjliu/openvla-7b\"\n",
    "# lora_ckpt = \"/remote-home1/pjliu/openvla/weights/select_fruit+CSv1+lora/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ[\"MUJOCO_GL\"] = \"glfw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the task episodes by seeds, instead of episodes\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(\n",
    "    tasks=demo_tasks,\n",
    "    n_episodes=2,\n",
    "    max_substeps=10,   \n",
    "    save_dir=save_dir,\n",
    "    visulization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load basic random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating select_fruit of RandomPolicy: 100%|██████████| 2/2 [00:27<00:00, 13.63s/it]\n"
     ]
    }
   ],
   "source": [
    "random_policy = RandomPolicy(model=None)\n",
    "result = evaluator.evaluate(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load policies, take OpenVLA as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = OpenVLA(\n",
    "    model_ckpt=model_ckpt,\n",
    "    lora_ckpt=lora_ckpt,\n",
    "    norm_config_file=os.path.join(os.getenv(\"VLABENCH_ROOT\"), \"configs/model/openvla_config.json\")\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation on different VLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VLABench.evaluation.model.vlm import *\n",
    "from VLABench.evaluation.evaluator import VLMEvaluator\n",
    "\n",
    "vlm_name = \"GPT_4v\" # valid names: [\"GPT_4v\", \"Qwen2_VL\", \"InternVL2\", \"MiniCPM_V2_6\", \"GLM4v\", \"Llava_NeXT\"]\n",
    "fewshot_num = 0\n",
    "task_list = [\"mesh_and_texture/select_fruit\"]\n",
    "\n",
    "def initialize_model(model_name, *args, **kwargs):\n",
    "    cls = globals().get(model_name)\n",
    "    if cls is None:\n",
    "        raise ValueError(f\"Model '{model_name}' not found in the current namespace.\")\n",
    "    \n",
    "    return cls(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm = initialize_model(vlm_name)\n",
    "evaluator = VLMEvaluator(\n",
    "    tasks=task_list,\n",
    "    n_episodes=2,\n",
    "    data_path=os.path.join(os.getenv(\"VLABENCH_ROOT\"), \"../dataset\", \"vlm\"),\n",
    "    save_path=os.path.join(os.getenv(\"VLABENCH_ROOT\"), \"../logs/vlm\"),\n",
    ")\n",
    "\n",
    "evaluator.evaluate(vlm, few_shot_num=fewshot_num)\n",
    "result=evaluator.get_final_score_dict(vlm_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlabench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
